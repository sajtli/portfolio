# ğŸ“ Offline Text Summarizer with LLMs

A fully offline, local-first text summarization tool powered by open-source large language models (LLMs) like Mistral â€” no APIs, no cloud, no internet needed after setup.

## ğŸš€ Project Overview

This project demonstrates how to build a usable, fast, and entirely local text summarization pipeline using quantized LLMs via [Ollama](https://ollama.com/). It's designed as a data science portfolio project with a focus on:

- Local deployment & LLM performance
- Practical prompt engineering
- Multimodal input support (future)
- Evaluation and explainability (future)

---

## ğŸ”§ How to Run It

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/offline-text-summarizer.git
cd offline-text-summarizer

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

### 3. Install dependencies

```bash
pip install -r requirements.txt

ğŸ’¡ Currently empty â€” will be updated in future versions

### 4. Install and setup Ollama

Download from https://ollama.com/download

Install the Mistral model:

```bash
ollama run mistral

### 5. Run summarizer

```bash
python summarizer.py sample_data/long_article.txt


## âœ… Features (Phase 1)

 Summarize plain text files via terminal

 Works completely offline after setup

 Uses local Mistral model via Ollama


## Roadmap

| Status | Feature                                              |
| ------ | ---------------------------------------------------- |
| âœ…      | Terminal summarizer with local Mistral model         |
| ğŸŸ¡     | Streamlit or Gradio-based GUI                        |
| ğŸŸ¡     | PDF input support                                    |
| ğŸŸ¡     | Web article scraping (offline-safe)                  |
| ğŸŸ¡     | Bullet-point vs paragraph summaries                  |
| ğŸŸ¡     | Evaluation with ROUGE & readability scores           |
| ğŸŸ¡     | RAG (retrieval-augmented generation) for long texts  |
| â¬œ      | Explainability (sentence importance, heatmaps)       |
| â¬œ      | Dockerized API version                               |
| â¬œ      | Hugging Face Space deployment (optional online demo) |
| â¬œ      | Blog post walkthrough                                |


## Folder structure

offline-text-summarizer/
â”œâ”€â”€ summarizer.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ sample_data/
â”‚   â””â”€â”€ long_article.txt
â”œâ”€â”€ models/
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


## ğŸ§  Notes
The model is run via subprocess calls to ollama run mistral for simplicity.

You can swap in other Ollama-supported models like llama3, gemma, or codellama.

Later versions will use llama-cpp-python directly for full control and speed.


## ğŸ‘¤ Author
Built by Tamas David Horvath â€” Mechatronical Engineer, Data Scientist & LLM Enthusiast
